---
title: "p8105_hw5_wc2750"
author: "Weixi Chen"
date: "11/16/2020"
output: github_document
editor_options: 
  chunk_output_type: inline
---

```{r setup, include = FALSE}
library(tidyverse)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
Read in the data
```{r message = FALSE}
homicide_df = 
  read_csv("homicide_data/homicide-data.csv") %>%
  mutate(
    city_state = str_c(city, state, sep = "_"),
    resolved = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>%
  select(city_state, resolved) %>%
  filter(city_state != "Tulsa_AL")
```

Let's look at this a bit
```{r message = FALSE}
aggregate_df = 
  homicide_df %>%
  group_by(city_state) %>%
  summarise(
    hom_total = n(),
    hom_unsolved = sum(resolved == "unsolved")
  ) %>%
  arrange(hom_total)

aggregate_df
```

Can I do a prop test for a single city?
```{r}
prop.test(
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_unsolved),
  aggregate_df %>% filter(city_state == "Baltimore_MD") %>% pull(hom_total)
) %>%
  broom::tidy()
```

Try to iterate...
```{r}
results_df = 
  aggregate_df %>%
  mutate(
    prop_tests = map2(.x = hom_unsolved, .y = hom_total, ~prop.test(x = .x, n = .y)),
    tidy_tests = map(.x = prop_tests, ~broom::tidy(.x))
  ) %>%
  select(-prop_tests) %>%
  unnest(tidy_tests) %>%
  select(city_state, estimate, conf.low, conf.high)
```

```{r}
results_df %>%
  mutate(city_state = fct_reorder(city_state, estimate)) %>%
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

## Problem 2
Import and tidy cohort dataset
```{r message = FALSE}
cohort_df = 
  tibble(
    path = list.files("lda_data")
) %>%
  mutate(path = str_c("lda_data/", path),
         data = map(.x = path, ~read_csv(.x))) %>%
  unnest(data) %>%
  mutate(id = str_replace(path, "lda_data/", ""),
         id = str_replace(id, ".csv", "")) %>%
  select(-path) %>%
  mutate(arm = ifelse(str_detect(id, "con"), "control", "experimental")) %>%
  relocate(id, arm)

# preview cohort_df
cohort_df
```

Make a spaghetti plot
```{r}
cohort_df %>%
  pivot_longer(week_1:week_8,
               names_to = "week",
               values_to = "observation") %>%
  mutate(week = str_replace(week, "week_", "")) %>%
  ggplot(aes(x = week, y = observation, group = id, color = id)) +
  geom_line() +
  facet_grid(.~arm) +
  labs(title = "Observations on each subject over time") +
  theme(plot.title = element_text(hjust = 0.5))
```

For control group, the observation values of all subjects keep stable along with an average value in `r cohort_df %>% pivot_longer(week_1:week_8,names_to = "week", values_to = "observation") %>% filter(arm == "control") %>% summarize(average = mean(observation)) %>% pull(average) %>% round(2)`  with some fluctuations over the 8 weeks; For experimental group, the observation values are generally above 0 and gradually increase from an average in `r cohort_df %>% filter(arm == "experimental") %>% summarize(average = mean(week_1)) %>% pull(average) %>% round(2)` at week 1 to an average in `r cohort_df %>% filter(arm == "experimental") %>% summarize(average = mean(week_8)) %>% pull(average) %>% round(2)` at week 8.

## Problem 3
First generate 5000 datasets with μ = 0
```{r}
# creat function for t test
t_test = function(u){
  vec = rnorm(30, sd = 5, mean = u)
  t.test(vec, alternative = "two.sided") %>%
    broom::tidy() %>%
    select(estimate, p.value)
}
```

```{r}
# simulate 5000 datasets with μ = 0
set.seed(123)
sim_results0 = 
  rerun(5000, t_test(0)) %>%
  bind_rows

# preview sim_results0
sim_results0
```

Repeat the above for μ={1,2,3,4,5,6}
```{r}
set.seed(123)
sim_results =
  tibble(sample_mean = c(1,2,3,4,5,6)) %>%
  mutate(
    output_lists = map(.x = sample_mean, ~rerun(5000, t_test(.x))),
    estimate_dfs = map(output_lists, bind_rows)
  ) %>%
  select(-output_lists) %>%
  unnest(estimate_dfs)

# preview sim_results
sim_results
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of μ on the x axis
```{r}
sim_results %>%
  group_by(sample_mean) %>%
  mutate(rej_prop = sum(p.value<0.05)/5000,
         sample_mean = str_c("μ =", sample_mean)) %>%
  ggplot(aes(x = sample_mean, y = rej_prop, color = sample_mean)) +
  geom_point() +
  labs(x = "Sample mean", y = "Rejection proportion", title = "Power of the test vs True μ") +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "none")
```

Based on the plot, we can see that with increased value of sample mean, the rejection proportion increases and approaches 100%. Generally speaking, when the effect size based on the difference in averages increases, the power of the test increases.

Make a plot showing average estimate of μ^ vs true μ
```{r}
all_estimate_plot = 
  sim_results %>%
  group_by(sample_mean) %>%
  mutate(average_est = mean(estimate),
         sample_mean = str_c("μ =", sample_mean)) %>%
  ggplot(aes(x = sample_mean, y = average_est, color = sample_mean)) +
  geom_point() +
  scale_y_continuous(breaks = c(1,2,3,4,5,6)) +
  labs(x = "Sample mean", y = "Average estimate of μ^", title = "Average estimate of μ^ vs True μ") +
  theme(plot.title = element_text(hjust = 0.5, size = 12), legend.position = "none")
```

Make a second plot showing average estimate of rejected μ^ vs true μ
```{r}
reject_estimate_plot = 
  sim_results %>%
  group_by(sample_mean) %>%
  filter(p.value<0.05) %>%
  mutate(average_rej = mean(estimate),
         sample_mean = str_c("μ =", sample_mean)) %>%
  ggplot(aes(x = sample_mean, y = average_rej, color = sample_mean)) +
  geom_point() +
  scale_y_continuous(breaks = c(1,2,3,4,5,6), limits = c(1,6)) +
  labs(x = "Sample mean", y = "Average estimate of rejected μ^", title = "Average estimate of rejected μ^ vs True μ") +
  theme(plot.title = element_text(hjust = 0.5, size = 12), legend.position = "none")
```

```{r}
# patchwork
all_estimate_plot + reject_estimate_plot
```

For μ = {1,2,3}, the sample average of μ^ across tests for which the null is rejected is greater than the true value of μ since their effect size is small and less proportion of extreme estimates is rejected; For μ = {4,5,6}, the rejected estimates are approximately equal to the true μ since these samples have large effect sizes with nearly 100% rejection proportions.